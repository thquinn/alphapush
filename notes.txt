- CPU/GPU issues
    - Running the network just doesn't take that long, so using the GPU is currently a waste.
        - That means parallelizing MCTS runs to batch forward passes is also not helpful.
    - The CPU is the bottleneck
    - Surprisingly, movegen isn't that bad. Right now the killer is converting states to tensors
        - Maybe I shouldn't be surprised: replacing random playouts with value predictions hugely reduces the load on movegen
        - Each modification to the tensor, or concatenating the whole thing, is taking way longer than expected
        - Improved this by using numpy arrays as an intermediate
        - Still taking longer than you'd think! Needs further optimization
        - Should probably use np all the way through PFState etc
        - In general, we'll want to store game states in a way that's easy for both movegen and conversion to network input
    - I'm realizing that you either have to train on CPU, or do a lot of shuffling of tensors around: gotta make parallel worth it somehow
- 750K (160, 512, 512, 512, 807)
    - It definitely learned to place pieces on the line, and usually to move them away from the edge
    - Most of the bugs seem to be fixed, but there's still the occasional do-nothing game where the agents somehow can't see the win-in-2.
    - These games seem like they take longer than usual to reach 200 history.
    - 750K_v006 policy priors are in the approximate range [-3, 3]: since these are multipliers, they should really be non-negative.
        - This is resulting in a very narrow MCTS.
    - Now using most robust child (best move only) in test matches instead of weighted by visits.
        - Oop, wait: we can't actually do this, since MCTS and the networks are both deterministic. We could try lowering the temperature...?
    - The network is probably just not deep enough. Let's do a run with a bigger net...
    - I'd forgotten to apply a softmax to the policy logits when instantiating MCTS: oops!
        - Also forgot to flip everything if the root state is black to play
- 1700K (160, 1024, 512, 512, 256, 256, 512, 807)
    - Upped exploration (aka c_PUCT in the AlphaZero paper) to 10, but it still seems like it's too low (now to 50)
    - Modifications made to network output before using them in MCTS need to be parallelizable.
        - Example: rotating the board for black is cute, but it means that half of the policy outputs need to be rearranged before storing in MCTS tree.
            - We could maybe parallelize this by sorting network inputs by state.white_to_play, so at least the policies that need rearranging are all together
            - But then it becomes annoying to get them back to the MCTS instance they belong to.
    - another round of profiling
        - CPU parallelism=1: generated 200 examples in 23.495507 seconds. (.117s per)
        - CPU parallelism=20: generated 4000 examples in 228.565861 seconds. (.057s per)
        - GPU parallelism=1: generated 200 examples in 39.540972 seconds. (.197s per)
        - GPU parallelism=20: generated 4000 examples in 343.205210 seconds. (.086s per)
- 1M (160, 512, 256, 256, 256, 256, 512, 807)
    - Lowered exploration back down to 5: if MCTS is largely driven by policy, then the result after running will be too similar and we have no delta to train on
    - Added momentum to SGD, lowered learning rate
    - check game states that are close to a win: make sure even a crappy network can find the win with enough playouts
        - white to play, has to move a round piece out of the way so a pusher can come in and win
            - 1M_v006 takes 1-2K playouts to see the line, fighting against policy: the move it lands on has 1%, and a non-winning move has 51%.
            - Uh oh: 1M_v009 needs 2-4K to see the line. At least policy is less confident about the wrong move at 49%...?
    - random thought: seems a bit of a waste to have training games and eval games. why not have all games be both?
        - round robin, train with jitter(?) on all games to create the next generation
    - 1Mv8 is about even with 1700Kv3, 65% against 750Kv6
    - Switching from 128 -> 1024 playout training games: ~12800 training examples in ~1000s. (NOTE: looks like I accidentally failed to change this.)
    - Suddenly, white is winning almost all promotion games. Are we overconcentrating our visits, causing duplicate games?
        - Promotion games are still only 128 playouts, which can be policy dominated.
        - Increasing playouts should also result in more unique games if only because it should result in longer games.
        - 1Mv9 vs 1700Kv3
            - @ 128 playouts: New network won 68 of 100 games (68.00%). 57 unique games. Average length 22.61. White won 80/100.
            - @ 256 playouts: New network won 70 of 100 games (70.00%). 63 unique games. Average length 26.93. White won 78/100.
            - @ 1024 playouts: New network won 80 of 100 games (80.00%). 67 unique games. White won 70/100.
        - It might not actually be that bad to have duplicate games, unless the reason is that the networks are stupidly losing immediately.
            - It's not great: even if we're seeing 60-70/100 unique games, they likely only differ at the very end
        - Switching to 256-visit promotion games: more playouts seems to give better signal
            - That's if you go by my biased definition of "better signal": the newer network winning more
    - 1M_v009 has learned that pushing is good, and isn't really moving much...
    - After v9, every new network is terrible, getting as low as 28% winrate.
    - A "network" that outputs all zeros is going to be better than early networks since it's vanilla MCTS, versus MCTS with random errors
        - We should use this to generate training data until a network can beat it
        - But is that true? Apparently not.
            - @ 256 playouts:
                - vs new network 1Mv1: New network won 55 of 100 (55.00%). 100 unique games. Average length 13.71. White won 69/100.
                - vs new network 1Mv9: New network won 81 of 100 (81.00%). 100 unique games. Average length 20.46. White won 51/100.
            - @ 512 playouts:
                - vs new network 1Mv1: New network won 58 of 100 (58.00%). 100 unique games. Average length 14.31. White won 68/100.
                - vs new network 1Mv9: New network won 83 of 100 (83.00%). 100 unique games. Average length 17.92. White won 49/100.
        - It's not really vanilla MCTS, since there are no actual rollouts, just zeros. Unlikely to reach winning states at much depth.
    - self-play is redoing a ton of work. can't we just build a single giant MCTS tree and any node with enough visits can serve as a training example?
        - would want to explore wider than a typical MCTS search since we're not just looking for winners, but really just any example
        - therefore we'd want to differentiate between visits made on UCT grounds vs. those just for funsies
        - we _could_ try to cut out some of the exploitation that hits leaf nodes over and over, but could also just optimize tree traversal
        - this gets us more policy examples, but what about value?
            - once we have enough policy examples, we can play those games out normally
        - thinking about it more: "a ton of work" is not accurate. the high branching factor of the game begs to differ.
            - but then again, if the policy output is opinionated, we can end up in the same sub-branches. see: our promotion matches containing duplicate games
- INVESTIGATION: check this position, where 1M_v009 at 20K playouts thinks White's best move is to move the left pusher to the edge on the bottom left:
      . . 
    . . .
    . w w .
    . b W .
    W B W B!
    . . B .
      . b .
      . .
    White to move | 1 moves left
        - of course, black then fails to see the win-in-two
			  . .
			. . .
			B w w .
			. b . .
			. B W!B
			. . W .
			  . . .
			  W b
        - The bad move is in third place with low policy prior and 1/4 the visits of the top, reasonable move
        - Black is also failing to see a win-in-1 at 20K playouts that it spots at 1K because bad moves are catching up with more playouts
            - It might just be that it's also a guaranteed win, and the slower move has a slightly higher policy prior
            - With 0 moves it only has two push options, and value is set correctly. Win-in-1 had a 40/60 incorrect policy.
            - Seeing two problems even at 20K playouts
                - Extremely high values getting overpowered by high policy priors
                    - With ~50 root children, about half are only getting a single visit. We want a higher exploration factor, but then the policy is too extreme
                    - The policy of 1M_v9 is just too extreme
                        - [(Move@5>21, 9639, '0.97', '0.13%'), (Move@25>13, 5747, '0.04', '61.19%'), ...
                        - The top move is a guaranteed win and the second is essentially a draw, but because it has 470x the policy prior, it is still eating half the visits
                        - Logits are 5.22 and -0.89, respectively. e^x ain't gonna fix that. How did this logit get so high?
                        - There's even a 7.66 logit, but it seems to be for an illegal move.
                        - Of course, if the high logit were for a good move, I'd be thrilled to see it.
                        - If we're fairly confident about the value, we should stop caring about the policy prior.
                        - Part of this is the network's fault, but part is the bound calculation
                - MCTS not finding a high expected value for a win-in-2 (three pushers ready to move into winning position)
                    - it's seeing it if it's literally the only move
                    - looks like this is just caused by the first problem: a non-winning move has a huge policy prior
                    - somehow, lowering exploration is making it worse
                    - okay, we were flipping value estimates from the network incorrectly
                    - win-in-2s aren't able to overcome low policy priors unless there are very few children
                    - tested with NullNet, which had all 3 winning moves top at 20K playouts, had to increase exploration to ~50 to make it "vanilla" MCTS
- 1M2 (same topology)
    - Hoping that the too-wide range of priors was the result of low-playout games which were dominated by policy priors, leading to a self-enforcing cycle.
    - Restarting with 1024 playout training, also fixed the incorrect value prior bug
        - Generated 6412 training examples in 2790s.
    - Re: policy logits: in training examples, pushes will always have high logits since they're sometimes the only legal moves. How do we prevent them from beating moves?
    - Terrible first run: 53%, 30%, 30%, 41%, 34%, 36%. All unique games.
- 320K
    - Last run was so disastrous, need to be able to iterate faster
    - Trying to switch to cross entropy or K1
    - First eval 46%, policy loss is on the rise. Could still be correct: better value estimates mean better policy to compare against.
    - Then down to 38%: loss is actually looking better than the start now, but it's seemingly not translating to good play.
    - Ran on one batch of training data, value and policy loss were indeed both going down. Might be an MCTS bug.
    - Removed softmax on policy logits in MCTS, using e^x instead, set exploration to the more standard sqrt(2).
    - Still consistently getting sub 40% with no wins. At least it's reproducible!
    - Overnight checkpoint performing about the same as 320v1 at the triple black pusher test case
        - But the slightly different case .....Bww......BWB..W.b..Wb has much worse policy priors. The white version doesn't have this problem.
    - Found a bug in PFState.to_tensor
    - Now reverting to old network on promotion match loss
    - Policy loss is too low
    - 320K_bad is placing its pieces on the edge
    - It also has hefty priors for insta-losing moves
        - These priors persist even after flipping colors, suggesting that flipping isn't to blame. Testing anyway.
    - Wait... does flipping the policy vector for push moves actually work? It should, given the (up, left, right, down) order.
    - Got rid of board flipping: no reason for the network to learn that your own pieces are usually at the top.
        - Maybe actively worse: flipping removes many symmetries, which AZ adds intentionally
    - Maybe failing to find a v2 network is expected behavior: a close-to-zero network is unbiased no-rollout MCTS, lets win values control the entire search
    - We should generate training data from NullNet until the network is sufficiently improved to make the slowdown worth it
    - Profiled NullNet generation: large percentage of time spent just in PFMove.place() calling cls(...)?
        - Current: Generated 6422 training examples in 416s.
        - Trying to use __slots__, slots=True, NamedTuple, etc was all slower somehow
    - Reading the AlphaZero paper:
        - their self-play games are from the latest network
        - training was learning rate 0.2->0.02->0.002->0.0002, 700K batches of 4096 examples @800 playouts
        - self-play was done on TPUs
        - root nodes added Dirichlet noise to policy priors
        - from the AlphaZero "game balance" paper: looks like it's just temp=1 for 20 ply, then temp=0
    - Even after 6 low-percent "wins" against previous networks, 320Kv7 is only 50% against NullNet
    - 320Kv7 @4k playouts as Black after a good placement from White wants to place its pieces at 25, 24, and 23, with negative priors, but high expected value
    - @512 playouts, training games are just very poor quality from NullNet, and in general. we need ~2k before we see even a glimmer of intelligence.
        - temp = 1 for training games is just wrong: we should pick the highest visit count much more often
        - lesson: actually inspect training data
        - Updated to 2048 playouts (still using NullNet to train): Generated 6409 training examples in 1626s.
    - Reading the KataGo paper:
        - They might just be using policy priors instead of e^x? And they have exploration at 1.1.
        - Also Dirichlet noise
        - Started with a smaller network, retrained each larger network on the same data until it caught up in loss
        - SGD with 0.9 momentum, batch size 256
        - "Playout cap randomization": Instead of always running a full MCTS in self-play, 75% of the time they use 1/5 the playouts, but don't use those outcomes as training data.
            - These fast moves disable noise and minimize exploration
        - "Forced Playouts and Policy Target Pruning": after N playouts in self-play, continue running until all root children have a minimum visit count
            - Then, modify the policy target to diminish moves that are still bad after this forced expansion
        - "Policy Surprise Weighting: examples can appear multiple times in the dataset, proportional to how much their observed policy dist. diverged from the prediction
        - Their network has another policy head for expected response moves from the opponent, which is only used in self play.
    - Value loss is still basically not moving even after hundreds of batches. There are enough obvious cases that the net should be able to learn: something bad is happening.
        - Okay, here's a problem: when generating examples, I'm treating the value output as "did the current player win (1), or lose (-1)?"
        - But in MCTS, it's "did Black win (-1), or White win (1)?"
        - Obviously we need to treat these consistently, and besides, the network doesn't even know what "White" and "Black" are.
- Testing mate-in-one-detection net on 1M examples (about 1 in 7 are mates in one)
    - SGD is still faster than Adam
    - Starting to seem like it's pretty inefficient running backprop on each training example just once
        - KataGo also keeps a "window" of examples from prior nets around.
            - The farther into training, the larger the window can be since even older models + playouts are stronger than the current net.
            - AlphaGoZero used a simpler window of just the most recent 500K games.
    - 117K batch1024 lr0.2: 0.0004 after ~40 epochs
        - lr decay .99 ** epoch: .00033 after 60 epochs
        X batch128: .00026 after 50 epochs, but twice as slow as batch1024. reverting...
        X I was missing a ReLU after the first layer (also in the "production" net) but adding it isn't having the effect I'd have imagined
        - batchnorm (up to 119K params): .00026 after 25 epochs, .0002 after 40, .00018 after 60
            - batchnorm theoretically goes before nonlinearity, but it's working better for me after, which some have observed: yet another hyperparameter
        X 3 more middle layers (up to 152K params): worse than the previous run
        X removing the last layer (91K params): quick to .0025 after 20 epochs, then starts flailing
        - shrink and restructure (64K params): .00023 at 40 epochs, sticks
        X try a huge run (811K params, batch128): overfit...? about the same as previous bests
        X training at half precision brought loss up by about 25%, not too bad, but no more performant
- Testing separate network trained on NullNet dataset: 270K_v000
    - Generating ~1K examples / 80 - 90 seconds
    - 20K examples can get test loss from 1.63 to 1.51
    - batchnorm is doing something screwy, making loss explode to infinity. Could just be that the dataset is too small...?
    - 100K examples, loss down to 1.36, but it still rebounds and starts increasing to 1.7+ after ~15 epochs: probably overfitting
        - Train and test loss descent together, then start to diverge as train loss plummets to 1.10
        - batchnorm seems to make it even better at overfitting, train loss 0.82 and falling while test loss > 1.6. At least test policy loss is decreasing! (2.8 -> 2.7)
        X reduced to 208K params, still overfitting. does reach 1.39 test loss, though. (reverted)
    - 500K examples, test loss to 1.32
        - 20% dropout between all hidden layers: test loss to 1.30 after 25 epochs, overfitting dramatically reduced
        - learning rate currently 0.01, if I increase it to 0.1 everything explodes (supposedly dropout lets you 10x LR but...)
        - lr 0.01->0.02, momentum re-enabled: test loss to 1.28 after 25 epochs
        X tripled network to 800K params: similar performance, slower to converge
        - a tiny win for Adam: on 800K params and 50% dropout, converges faster than SGD! still no better than 270K, reverting...
        - loaded up 1Mv9 out of curiosity: it has higher test loss than random
    - Testing null training results: 270Kv0 beats NullNet 64% @512 playouts, 74% @2K, 91% @8K!
    - Spot-testing individual states
        - Strong policy/value opinions haven't percolated up to White's placement moves yet
            - policy is between -0.01 and -0.05 for all first moves
            - value is seemingly 0.31 for everything...? even @1M playouts, no real sign of higher values. visits equal across all root children
        - Signs of life policy-wise: winning pushes with ~1 policy, losing ones with negative policy
- Optimizing self-play
    - Oops! We were still flipping the anchor position on black to play.
    - MCTS.receive_network_output is 12%, PFState.set_moves 26%, MCTS.get_current_state_tensor 20%
    - Before optimization: Generated 109 examples in 136.1 seconds.
    - After optimization: Generated 133 examples in 91.3 seconds.
    - "Full" run: Generated 10017 examples in 4873.4 seconds. Saving...
    - Batching NN execution results in 50-60% CPU utilization, but is still faster than the multiprocessing.Pool used for nullnet data gen which totally maxes CPU
- Trying to train 270Kv1...
    - Self-play is slow, so trying 110K new examples mixed with 110K old
        - Could be worth cycling the old examples every epoch to reduce overfitting...?
    - Overfitting is obviously a problem, so increased dropout to 50% which seems to just baaaaaarely be doing the job
    - Test loss was 1.28 at end of v0 training, now 1.39
    - We can really only get test loss down to 1.34 with this little new data.
        - The good news (maybe?) is that a higher percentage of this loss reduction seems to be coming from policy
    - Playing a promotion match with this crappy 270Kv1
        - I've learned that the only guaranteed losing move during piece placement is Black's Place@17, and i've seen it 6-1 from v0-v1
        - Incredibly, after just 100 epochs on a 220K half-null half-v0 dataset resulting in just 3.5% test loss reduction, we have a result:
        - v1 beats v0 66% @2K playouts! v0 still won 24/50 games as White.
        - Using very rough math, that's +304 Elo @2K playouts over NullNet.
    - I'm starting to see why promotion matches are run so frequently in AGZ, and not at all in AG: the network improves fast, so you want it to be the one self-playing.
    - Another detail from the AGZ paper: temperature starts at 1 during self play, but tapers to 0
    - More spot testing
        - I can still beat it in just a few turns @20K. No blunders, just bad positioning.
        - Empty board @2K now has policy from .23 to -.19 — this spread using e^x is only 144% max/min — values still uniform (0.23)
        - Empty board @1M still has basically no value spread, uniform at -0.16... the more it thinks, the more it expects White to lose from the start...?
        - Black's first placement vs. a standard white placement does indeed dislike Place@17, -0.30p vs. its favorite Place@16 with 0.56p
        - Black start @100K has uniform value -0.77 except Place@16 which has 0.34v and 10x the next move's visits with just 0.28p more.
            - The trend is even worse @200K, Place@16 now has 30x
        - Losing black Place@17 in promotion game position ....W.w.Ww..W..B..B....... was indeed just temperature, last @2K and @100K
            - Still has positive average value @100K, strangely...
        - Bad black move in promotion position Ww..WwB...WB....B..b..b... was also temperature
            - Ditto white in ......WW.w.wW.B.BB.b..b... w/ 2 moves
        - Promotion position PFState.construct('...w.WB..w..Bb....W.WBb...', False, 2, 18) is a disaster for v1, Push@6.Up has -0.25p but somehow 0.51v @8K, still 0.42v @100K (!!)
            - @100K: still liking its chances with Move@12>14, Move@22>11, but then hates its only move Push@6.Up. This looks like an MCTS bug...?
            - It thinks that White will respond immediately with Push@5.Down, probably because it thinks there's 0.30p for that move. But is there...?
            - No, it looks like it has 0.20p in that state. Are we passing the wrong state to the network, or doing something wonky with the output?
            - We were always passing the root state tensor to the net in debug_state.
        - Vs. me, PFState.construct('.........wwWW.BbB..B.W..b.', False, 2, 21) it does save the piece, but then pushes it down to 23
            - But now in a spot check, after Move@24>20 it opts for Push@16.Left. In the game it kept visits from previous moves...
            - Even then, how did Push@16.Down overtake it? It has bad policy in both preceding states and in spot eval is in dead last in that position
            - Probably just the root tensor bug again: it's going for Push@16.Left @2K
            - Uh oh, back to Push@16.Down @20K: 0.02p, but it's tied for second in value
            - Maybe it's a "logical" move to the network: the anchored pusher is exactly where White would need to push from to win.
            - But now @100K we've landed on the disastrous Move@14>17 with -0.30p and somehow the highest value.
            - For the final move of its turn, Black's value estimate has fallen from 0.27 to -0.05
            - Likely a horizon problem: after Push@16.Left, White needs its full turn to win (lots of options, though...)
            - It thinks White's three most likely moves are getting the pusher at 21 to safety. It has a certain logic...
            - The model may just not have the parameters/depth to recognize 'flood fill' regions like this.
                - Ideally the policy for White's first move would be high for Move@9>anywhere.
- 270Kv2
    - Training with 110K examples from v0 and 210K from v1
        - Test loss starting at 1.48
        - batch512 is settling around 1.38/1.43 train/test
            - That includes the biggest drop in policy loss we've seen from 2.6768 to 2.6561
            - In the promotion match, they both seem stupider @2K than before: repeatedly pushing their own pieces around
            - But then somehow it wins 70%...
        - Trying a 600K version: 1.36/1.43 after 400 epochs, only wins 39% against 270Kv2 @256
        - Argh, we can't be using unseeded randperm to mix datasets
    - Alright, we had a bug in debug_state (ha, ha), time for more spot checks
        - white start policy spread from -0.40p to 0.32p
            - the flat values we were seeing was the "always passing root state" bug: even v0 has a spread from 0.21v to 0.38v @2K
        - lethal Black Place@17 policy has risen slightly to -0.25p, MCTS still kills its value though
        - 'antipolicy' still better for NullNet presumably because it requires moving a piece to a risky spot
        - 'horizon6' still too hard for v2: black has two moves and a push to go, then it takes all 3 actions from white to win
            - @320K it's starting to see that White likes its chances: RAVE would probably help here
        - 'deadlock': signs of life @100K! White's winning Move@9>6 is in the principal variation with positive policy.
            - And indeed, @200K, we've got the totally palatable Push@16.Left at the top and the blunder 3rd.
        - the bug was probably in versus() also, but v2 still wins 65% against v1. That's ~414 Elo over NullNet @2K now!
            - And just 66% @8K, so I guess that's not a trend after all.
- GCloud
    - each C2D($67/mo) machine takes ~2h to generate 10K examples using 1/2 cores
    - T2D's ($32/mo) takes ~2h15m for 10K examples. 1vCPU indeed hits 100% CPU with normal training. i can bring up 8 of these in one region!
    - N2D ($50/mo) takes ~2h15m for 10K examples, hits 50% CPU
    - With spot instances, T2D costs about 1.8 cents per 10K examples generated. That $300 credit is worth ~167M examples!
- 270Kv3
    - Training with 470K examples from v2. Starting test loss: 1.5600.
        - Seems like this should be enough data for 20% dropout + 1024 batch size.
        - Not surprised to see a significantly higher starting test loss here: this is our first training round with all new data. (Plus the data is harder.)
        - Oops, this was with lr=.002, but: 1.5003/1.5200 after 200 epochs.
        - lr=0.2
            - Epoch 10 finished in 4.8s. Train/test loss: 1.4833/1.5159
            - Epoch 50 finished in 4.6s. Train/test loss: 1.4510/1.5300.
            - Alright, so a higher learning rate is still on the table, but we're also still overfitting.
        - lr 0.02, 50% dropout, batch128, loaded from epoch 10 above. This is weird:
            - Start: Test value loss: 0.8538, test policy loss: 2.6486.
            - Ep.45: Test value loss: 0.8694, test policy loss: 2.6445.
            - Value loss was fluctuating and trending upwards, but policy loss was steadily decreasing the entire time. A sign that the network is too small?
            - Could also be the noisy value signal vs. the smooth policy signal
            - Also, training totally froze at epoch 46 with the GPU reporting 100% usage, but holding at 50C (basically idle temp)
        - The 1.5600 -> 1.5159 loss 270K network only got 53% against v2.
        - Tried an 900K model with lr 0.002, hugely overfitting with 20% dropout and batch128
    - Up to 710K examples and 50% dropout on a 900K param network
        - Slows down overfitting and we can edge out our previous best 270K net (1.5151 after 40min), but training loss keeps falling after test loss levels out
    - Alright, 270K was able to get there with more examples and lr=0.002 50%drop batch128: 66% vs 270Kv2 (+532 Elo over NullNet)
        - Even though it only got to 1.5-something test loss, I think that magnitude of loss improvement is still enough: the previous 1.5159 was likely just a fluke
        - v3 beats NullNet @256 92%: +401 Elo, by that result.
    - Spot testing
        - 'goodenough' has the net fighting low policy, 52 branching factor, and a move that's basically a lock to miss the win-in-3. NullNet can't find it either...?
            - it's really the policy network's job to ensure prioritizing good moves even when you're winning, so we'd want to see good policy here
        - 'lockin': strange to see -0.21p for a great looking move
    - MCTS overhaul for max-max: "extended win propagation"
        - if a won state is found, all previous moves during that player's turn are essentially won as well, and their values should just be 1.
        - unclear exactly what to propagate
            - whatever the visit count of that turn's first move was, we want to convert that much value from all ancestors to "win"
            - seems weird, but we've finally realized that it IS a win, and we HAVE visited it that many times, so the stats are now wrong... right?
        - The same-turn-only version of this ended up being either neutral or slightly negative in testing: not sure why.
- 270Kv4
    - Starting test loss: 1.5363. That's gotta be our lowest yet.
    - We're also switching to not measuring loss on the entire training set post-epoch, since it was overrunning VRAM, so we'll see higher training loss in logs.
        - We should even expect to see higher training loss than validation loss in early epochs.
        - The larger the dataset / smaller the batches, the more pronounced this will be since we're averaging training loss over older and older batches
        - Probably an even bigger source of loss increase is that now we're measuring in train() mode, where dropout is enabled.
        - I can still recognize when it's overfitting, so that's good! As learning slows down, averaging over old batches doesn't matter as much, and we can see the gap
    - Still getting ~0.87 value loss, which makes sense when all of the examples are -1 or 1, but most positions should be in the -0.25 to 0.25 range
    - rc1: Epoch 130 finished in 19.3s. Train/test loss: 1.5104/1.5197.
        - At lr=.002, seemed like some progress was still being made without too much overfitting
        - Upping the learning rate seems to result only in more overfitting
        - After more attempts, it seems like you do get faster/better results training the previous model version than starting fresh.
        - In fact, it's not clear that fresh starts would ever reach the same test loss: is this a kind of accidental "curriculum"?
        - Have to be careful with these comparisons if we start joining datasets, since we'll be using previous test data to train
    - 900K is still too good at overfitting on <1M examples, test loss lagging behind 270K
    - Trying a 320K version: Epoch 98 finished in 30.1s. Train/test loss: 1.5196/1.5322.
        - Doesn't seem like we're going to get much below 1.53 this way.
        - Tried PReLU and 20%drop, got to 1.5289 but it was overfit and test loss was unstable.
    - rc2: Trying 320K, .002lr, batch128, PReLU, 20%drop again with v2+v3 datasets (1.5M): 80% VRAM, expecting the "easier" examples to result in lower loss overall
        - Yep! Epoch 5 finished in 66.6s. Train/test loss: 1.5462/1.5187. Certainly the more-than-doubled dataset also helps with test loss.
        - No overfit even after 40 epochs, we could maybe ditch the dropout altogether.
        - Even though I'm scrambling the test sets, I'm still seeing lower (last epoch test loss) than (this epoch train less), which is concerning. At least the gap is narrowing...?
            - This finally reverses around epoch 70. Maybe the test split just happens to be a little easier, combined with underfitting?
        - It's a win/win to see this lower overall loss, since it either means:
            - that learning is improving with the new model and larger dataset, or
            - that the v2 dataset is markedly easier, which implies that our data is improving
        - Train loss improvement is starting to slow around epoch 50, before it's even crossed over test loss. Looking underfit, though it *is* still decreasing.
        - Stopping: Epoch 73 finished in 68.8s. Train/test loss: 1.4888/1.4889. 0.8319 value, 2.6281 policy.
        - These small batch sizes are really reducing my GPU usage %.
        - Increasing learning rate 5x to .01 is having surprisingly little effect on the pace of convergence
    - rc3: Trying 900K yet again
        - A new best result, finally! Reaches 1.4695/1.4815 at epoch 40 with just 20%drop.
        - Looked much slower to train at first, but it was just VLC running and (presumably) competing for VRAM.
        - It seems like in low-to-mid-overfit scenarios, test-value loss thrashes more than test-policy loss. Policy just has more signal?
        - Stopping: Epoch 47 finished in 76.0s (73.1s batch time). Train/test loss: 1.4648/1.4797. 0.8234 value, 2.6251 policy.
    - A little inter-candidate eval:
        - rc1 vs rc3 @256: with triple the parameters, rc3 should thrash rc1 on policy alone, right? Apparently not: rc1 wins 56%.
        - rc1 vs rc2 @256: seems like rc2 won't stand a chance then, if the dataset recency matters that much? Well... rc1 wins 53%, within variance.
        - The larger models don't take much longer to train, but they do take noticably longer to play, even 320K vs 270K params. Deeper networks must hurt the CPU.
    - Back to 270K with more data: 1120K examples from v3. Trying to beat 1.5197 test loss.
        - Our starting test loss vs 640K examples was 1.5600, now 1.5260...?
        - lr0.01, 20%drop, 128batch: Epoch 51 finished in 33.1s (32.0s batch time). Train/test loss: 1.5051/1.5070.
        - continuing at lr0.02, 50%drop: Epoch 37 finished in 36.2s (35.1s batch time). Train/test loss: 1.5014/1.5069. Basically no gains. Still looks underfit.
        - Aaaand it only wins 43% vs rc1. It did have only 51 epochs of actual training vs the 130 of rc1, so... maybe that's why?
        - rc1 63% vs v3
    - Boy, we'd sure love to avoid setting moves on leaves until we're expanding it later, but we need to know if the current player has any legal moves, otherwise they lose
        - We could do it where moves_left > 0
    - Spot testing
        - state = PFState.construct('.....WWw.BB.w..WB..bb.....', False, 2, 6) is a weird case: even at 4K, v4 wants to move a piece and then move it right back
        - I'm still beating v4@20K in just a few moves
        - 'misplaced': Place@17 is experiencing a big resurgence, seemingly because the policy logits for all other Black placements are falling
            - It only seems to be happening in one position, on the third Black placement. v3 and v4 are roughly the same, but 900Kv4d is the worst with this.
    - One last Hail Mary for 900K params with our v003_1120K dataset, trying to beat 1.5069 train loss
        - Epoch 40 finished in 52.3s (50.2s batch time). Train/test loss: 1.5166/1.5152.
        - Adam took us from 1.7072 to 1.5090 in 12 epochs. (saved as *_4b)
        - Still with Adam, trying "curriculum learning" on v2 and v3 datasets: from 1.7033 to 1.5097 in 11 epochs on v2 (4c), then from 1.5221 to 1.5041 in 43 epochs (4d)
            - New policy highscore of 2.6088
        - Still loses against 270Kv4. Gonna keep it around and keep training it on the next set.
- v5
    - 270K: Starting loss 1.5073... that's low.
        - Hits a low of 1.4970 at e35, but training loss keeps going down for another 50+
    - 900K: Starting test loss: 1.4974. That's REAL low, not having much luck improving it.
    - 270K attempt 2: maybe batch128 is just too hard, getting smoother descent with 1024 and lr0.02.
        - Hits a low of 1.4959 after ~e100
        - Wins 60% against v4, still playing Place@17 but maybe actually looking smarter otherwise
    - Spot testing
        - 'misplaced': v3 -> v4 -> v5 visits on Place@17 are 266, 273, 273... at least the trend has stopped (or paused)
            - policy is higher, but so are most of the logits which is good since we don't want them to all be negative, right
        - changed 'horizon6' to 'horizon5': still plenty hard (apparently) and only has one right answer (Move@15>16)
    - 900K_v004d continues on dataset_270K_v004_1080K: Starting test loss: 1.5007. batch256 SGD lr0.02
        - maxes at Epoch 61 finished in 33.3s (31.4s batch time). Train/test loss: 1.5170/1.4921, then to 1.4918 after a few epochs of 20%drop.
    - Trying a fresh 670K network with no dropout and AdamW, which uses L2 regularization to reduce overfitting. Number to beat: 1.4959.
        - Train and test both leveled off high
        - Trying with 512batch and halved lr (0.01): overfitting!
        - Things were better back when we were running on larger batches, so let's try upping the L2 (0.02) and slowing lr again (0.002)... still overfitting like a champ
        - AdamW's weight_decay isn't doing much to combat overfitting, that I can see...
        - Can test if we get better results from progressing through v2 -> v4 datasets
    - Alright, forget Adam. More attempts at deeper architectures with SGD
        - A deep 215K net with no normalization had no problem overfitting on 1M examples, test loss bottomed at 1.5074.
        - Struggling to get good results from L2: could be in part that I'm masking most of the output, and so not updating those weights, and they're just constantly decaying.
        - Up to 512batch, testing weight_decay values
            - 0.0001 diverges after Epoch 10 finished in 26.4s (25.3s batch time). Train/test loss: 1.5031/1.5099.
            - 0.0005 also diverges after Epoch 10 finished in 26.9s (25.8s batch time). Train/test loss: 1.5088/1.5129.
        - Back to 20% dropout and... Epoch 54 finished in 32.5s (31.3s batch time). Train/test loss: 1.5247/1.5119. About the same as L2, takes way longer.
            - Test loss does linger near the minimum instead of rising, and occasionally dips slightly below.
        - Trying to retrain 270Kv4 on v4_1080K dataset
            - Dropout is just such an easy hyperparameter to work with: 20% works
            - Long run. Epoch 92 finished in 21.2s (20.2s batch time). Train/test loss: 1.5007/1.4972.
                - Then moderate overfitting for 900 epochs, test loss remains pretty stable. Epoch 1000 finished in 19.2s (18.4s batch time). Train/test loss: 1.4759/1.5042.
                - It's really just value that gets overfit, and I can't believe I haven't thought of this before. Best policy is reached in epoch 965: 2.5544.
                - But we only win 46% against v5: value loss is still important.
    - Assorted fun facts
        - LLaMa2 70B was trained on 28 examples per parameter.
        - Leela Zero had 47 million params
    - Maybe policy loss should be weighted a little higher, especially since it's the one that can be improved substantially (vs. the terrible value signal)
        - Value loss seems more important for search, actually.
- v6
    - 270Kv5 lr0.01 batch256 drop10%. Starting test loss: 1.4925.
        - The starting loss getting lower and lower makes me worry that policy is self-reinforcing via training data. Value seems the bigger problem right now though.
        - Smooth descent of train/test for 30 epochs, then a little bumpier for test but no real divergence until ~e60
        - Epoch 59 finished in 23.1s (22.0s batch time). Train/test loss: 1.4751/1.4801. New best policy loss of 2.5275.
        - Beats v5 61%.
    - 'misplaced': visits are down from 273 to 259, value from 0.29 to 0.27.
    - 900Kv5d lr0.01 batch256 drop20%: Starting test loss: 1.4859. Epoch 74 finished in 54.3s (50.8s batch time). Train/test loss: 1.4890/1.4796.
        - 38% vs v6, Place@17 up to 322 visits. Total disaster.
    - Trying 275K deep config with residual blocks, lr0.0002 20%drop: Epoch 59 finished in 35.8s (34.4s batch time). Train/test loss: 1.5086/1.4976. saved as test_v006a
        - lr0.002/1 through datasets 002-004: test loss 1.7075 to 1.5319 in v2, 1.5495 to 1.5121 in v3, 1.5075 to 1.4982 in v4
            - Quickly beat previous bests on v2 and v3 sets, but the previous best loss on v4 (1.4959) was from batch1024 and is tough to beat
            - Reached 1.4982 on batch256, only overfitting on batch1024. Also tried weight decay, batch4096, no dice.
            - Starting loss on v5 was 1.4896, lower than 270K's start, also lower than the "test_v006a" attempt's *end*
            - v5 lr0.001: 1.4896 to 1.4775.
        - Place@17 has 203 visits and the lowest policy, still in third though with the highest average value. Something's up with value...
            - Expecting it to crush 270Kv6 on this alone.
            - Shit, it played Place@17 in its first game as Black. And its second. Never mind.
            - 45%.
- v7
    - 270Kv6 lr0.01 batch256 drop10%. Starting test loss: 1.4744. Epoch 51 finished in 22.1s (21.1s batch time). Train/test loss: 1.4662/1.4695.
        - Only 51%, plus 'misplaced' has crept back up to 274 visits, and now has THE HIGHEST POLICY.
    - Alright, we had a bug with generating training examples: the signs were incorrect on the move when the player changed.
        - Hopefully this was the cause of the post-Place@17 values, and the high value loss in general.
        - v6 with White to play after misplacement is 0.73: still pretty high, despite all the -1 examples it's been shown.
            - Importantly, it's not negative, so still unclear where Black was getting all the high values from.
- v7 take 2, post value fix
    - 270Kv6 lr0.01 batch128 drop20%. Starting test loss: 1.6307. Starting test policy loss: 2.5264 (low), test value loss: 0.9991 (very high, could be a good sign!)
        - Makes sense that value loss essentially looked random, since all post-push moves previously had their sign flipped
        - Epoch 49 finished in 21.7s (21.0s batch time). Train/test loss: 1.4945/1.4917.
        - 'misplaced': second lowest policy, still the top move and highest value but just barely at 233 visits and 0.24v. Better direction...
        - 25%. Terrible direction! Almost every game it won was from v6 playing Place@17, and it played Place@17 itself many times, in new situations.
    - Did we have a bug? Looking at it more carefully, it seems like the original logic was correct.
        - Looks like the bug was in MCTS the whole time. So the training data is fine...?
        - The more accurate the value estimates were getting, the worse it would perform in MCTS.
    - Spot testing 'v6fix'
        - 'misplaced': v6 now HATES Place@17: dead last. This has to be the bug.
        - The regressions so far involve failed to find a faster win in an already won position.
        - 'singularity': Exactly the type of thing I'd hope MCTS could spot, and it finally does! The next batch of training data should be MUCH better.
- v7 take 3, post MCTS fix
    - 270Kv6 lr0.01 batch128 drop20%. Starting test loss: 1.4726. Epoch 27 finished in 46.1s (44.8s batch time). Train/test loss: 1.4719/1.4669.
    - Couple more at batch256: Epoch 5 finished in 26.7s (25.5s batch time). Train/test loss: 1.4652/1.4658. New best policy loss: 2.5196.
    - Found a minor bug in eval_match: draws weren't actually getting replayed, deducting against the new net's potential win count.
    - 59+% vs v6: both networks looking much stronger in the eval match.
    - 275K: Epoch 22 finished in 35.7s (34.1s batch time). Train/test loss: 1.4619/1.4656. New best policy loss: 2.5148.
    - 900K: This topology just ain't workin'. Discontinuing...
    - New residual "520K"
        - Fresh on v6: Epoch 42 finished in 33.6s (31.8s batch time). Train/test loss: 1.4485/1.4714.
        - lr0.002 batch256 drop10% curriculum. v3: 1.7129 -> 1.5129, v4: 1.5078 -> 1.4977, v5: 1.4890 -> 1.4791
        - Up to drop40% for v6: 1.4743 -> 1.4627 after 430 epochs. Ran for 737, test loss was surprisingly stable throughout moderate overfitting. Best policy loss: 2.5100.
- v8
    - 270Kv7 lr0.002 batch256 drop20%. Starting test loss: 1.5393. Epoch 151 finished in 21.6s (20.7s batch time). Train/test loss: 1.4834/1.4767.
        - Super smooth descent, could definitely up the learning rate.
        - 55%.
    - 520Kv7 lr0.01 batch256 drop20%. Starting test loss: 1.5392. Epoch 46 finished in 62.1s (58.6s batch time). Train/test loss: 1.4537/1.4592. Best policy loss: 2.4876.
        - 65% vs 270Kv8, uses ~60% more time
    - 270Kv7 lr0.002 batch256 drop20% with twice the training data. Starting test loss: 1.5409. Epoch 661 finished in 44.7s (42.5s batch time). Train/test loss: 1.4615/1.4620.
        - Another surprisingly smooth, long training session. I guess the learning rate could go up more?
        - 48% vs 270Kv8, despite a .0147 loss improvement, which has historically been significant. 
- TODO
    - expand children in policy order
    - separate loss.backward for value and policy, to handle value overfitting and policy underfitting?
    - taper temperature during self-play
    - how can i juice this thing before the exhibition
        - profile
        - improvements from AZ and KataGo papers: Dirichlet noise and playout cap randomization
            - Dirichlet noise could also help prevent the "exploding policy problem"
        - exploration might be too low given the conservative policy range (range is growing, though...)
        - at match time: can we use the GPU to evaluate a single position somehow?
        - or, at match time: grab the most powerful cloud CPU I can find and use that to run instead
        - could disallow moving the same piece twice in a row, to reduce branching...?
        - is most robust child still the way to go?
            - if we wanted to be very fancy, we could try extrapolating node stats from current trends
    - try half precision on the new GPU (requires tensor cores)
    - parallelize promotion games
    - seeing anecdotal evidence that the training values should incorporate the MCTS result, not just the game result
    - "curriculum" based on high-loss game states: KataGo kind of does this
    - per-net benchmarks like "% of cases where the best move after 1M playouts had the best policy logit"
- BLOG POST
    - At first tried to train from each game once, then threw it out. Huge waste!
    - Python is not good at running MCTS
        - But it almost doesn't matter, since even if you batch inference, all of your time is spent moving tensors to/from the GPU
        - The important lesson is that you just need a ton of data
    - Value-flipping bug: MCTS is still tricky to implement yet surprisingly resilient to error
    - Google Cloud tricks
    - Hyperparameters: network sizes, dropout vs L2, Adam/AdamW never really worked
    - Attempts at training larger nets, curriculum learning (is it just the larger corpus?)
    - Problems with my implementation
        - e^x in the exploration term
        - policy self-reinforcement
        - High temperature in self-play
        - Noisy value signal, noisy value descent compared to smooth policy
        - 2048-playout self-play games makes for slow iteration
        - Probably still some bugs: should have written tests and worked up from simpler cases.
    - Future improvements: continuous training, Dirichlet noise
    - References: "Simple AlphaZero," AGZ paper, AZ paper, KataGo paper, KataGo improvements page on GitHub