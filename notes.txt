- CPU/GPU issues
    - Running the network just doesn't take that long, so using the GPU is currently a waste.
        - That means parallelizing MCTS runs to batch forward passes is also not helpful.
    - The CPU is the bottleneck
    - Surprisingly, movegen isn't that bad. Right now the killer is converting states to tensors
        - Maybe I shouldn't be surprised: replacing random playouts with value predictions hugely reduces the load on movegen
        - Each modification to the tensor, or concatenating the whole thing, is taking way longer than expected
        - Improved this by using numpy arrays as an intermediate
        - Still taking longer than you'd think! Needs further optimization
        - Should probably use np all the way through PFState etc
        - In general, we'll want to store game states in a way that's easy for both movegen and conversion to network input
    - I'm realizing that you either have to train on CPU, or do a lot of shuffling of tensors around: gotta make parallel worth it somehow
- 750K (160, 512, 512, 512, 807)
    - It definitely learned to place pieces on the line, and usually to move them away from the edge
    - Most of the bugs seem to be fixed, but there's still the occasional do-nothing game where the agents somehow can't see the win-in-2.
    - These games seem like they take longer than usual to reach 200 history.
    - 750K_v006 policy priors are in the approximate range [-3, 3]: since these are multipliers, they should really be non-negative.
        - This is resulting in a very narrow MCTS.
    - Now using most robust child (best move only) in test matches instead of weighted by visits.
        - Oop, wait: we can't actually do this, since MCTS and the networks are both deterministic. We could try lowering the temperature...?
    - The network is probably just not deep enough. Let's do a run with a bigger net...
    - I'd forgotten to apply a softmax to the policy logits when instantiating MCTS: oops!
        - Also forgot to flip everything if the root state is black to play
- 1700K (160, 1024, 512, 512, 256, 256, 512, 807)
    - Upped exploration (aka c_PUCT in the AlphaZero paper) to 10, but it still seems like it's too low (now to 50)
    - Modifications made to network output before using them in MCTS need to be parallelizable.
        - Example: rotating the board for black is cute, but it means that half of the policy outputs need to be rearranged before storing in MCTS tree.
            - We could maybe parallelize this by sorting network inputs by state.white_to_play, so at least the policies that need rearranging are all together
            - But then it becomes annoying to get them back to the MCTS instance they belong to.
    - another round of profiling
        - CPU parallelism=1: generated 200 examples in 23.495507 seconds. (.117s per)
        - CPU parallelism=20: generated 4000 examples in 228.565861 seconds. (.057s per)
        - GPU parallelism=1: generated 200 examples in 39.540972 seconds. (.197s per)
        - GPU parallelism=20: generated 4000 examples in 343.205210 seconds. (.086s per)
- 1M (160, 512, 256, 256, 256, 256, 512, 807)
    - Lowered exploration back down to 5: if MCTS is largely driven by policy, then the result after running will be too similar and we have no delta to train on
    - Added momentum to SGD, lowered learning rate
    - check game states that are close to a win: make sure even a crappy network can find the win with enough playouts
        - white to play, has to move a round piece out of the way so a pusher can come in and win
            - 1M_v006 takes 1-2K playouts to see the line, fighting against policy: the move it lands on has 1%, and a non-winning move has 51%.
            - Uh oh: 1M_v009 needs 2-4K to see the line. At least policy is less confident about the wrong move at 49%...?
    - random thought: seems a bit of a waste to have training games and eval games. why not have all games be both?
        - round robin, train with jitter(?) on all games to create the next generation
    - 1Mv8 is about even with 1700Kv3, 65% against 750Kv6
    - Switching from 128 -> 1024 playout training games: ~12800 training examples in ~1000s. (NOTE: looks like I accidentally failed to change this.)
    - Suddenly, white is winning almost all promotion games. Are we overconcentrating our visits, causing duplicate games?
        - Promotion games are still only 128 playouts, which can be policy dominated.
        - Increasing playouts should also result in more unique games if only because it should result in longer games.
        - 1Mv9 vs 1700Kv3
            - @ 128 playouts: New network won 68 of 100 games (68.00%). 57 unique games. Average length 22.61. White won 80/100.
            - @ 256 playouts: New network won 70 of 100 games (70.00%). 63 unique games. Average length 26.93. White won 78/100.
            - @ 1024 playouts: New network won 80 of 100 games (80.00%). 67 unique games. White won 70/100.
        - It might not actually be that bad to have duplicate games, unless the reason is that the networks are stupidly losing immediately.
            - It's not great: even if we're seeing 60-70/100 unique games, they likely only differ at the very end
        - Switching to 256-visit promotion games: more playouts seems to give better signal
            - That's if you go by my biased definition of "better signal": the newer network winning more
    - 1M_v009 has learned that pushing is good, and isn't really moving much...
    - After v9, every new network is terrible, getting as low as 28% winrate.
    - A "network" that outputs all zeros is going to be better than early networks since it's vanilla MCTS, versus MCTS with random errors
        - We should use this to generate training data until a network can beat it
        - But is that true? Apparently not.
            - @ 256 playouts:
                - vs new network 1Mv1: New network won 55 of 100 (55.00%). 100 unique games. Average length 13.71. White won 69/100.
                - vs new network 1Mv9: New network won 81 of 100 (81.00%). 100 unique games. Average length 20.46. White won 51/100.
            - @ 512 playouts:
                - vs new network 1Mv1: New network won 58 of 100 (58.00%). 100 unique games. Average length 14.31. White won 68/100.
                - vs new network 1Mv9: New network won 83 of 100 (83.00%). 100 unique games. Average length 17.92. White won 49/100.
        - It's not really vanilla MCTS, since there are no actual rollouts, just zeros. Unlikely to reach winning states at much depth.
    - self-play is redoing a ton of work. can't we just build a single giant MCTS tree and any node with enough visits can serve as a training example?
        - would want to explore wider than a typical MCTS search since we're not just looking for winners, but really just any example
        - therefore we'd want to differentiate between visits made on UCT grounds vs. those just for funsies
        - we _could_ try to cut out some of the exploitation that hits leaf nodes over and over, but could also just optimize tree traversal
        - this gets us more policy examples, but what about value?
            - once we have enough policy examples, we can play those games out normally
        - thinking about it more: "a ton of work" is not accurate. the high branching factor of the game begs to differ.
            - but then again, if the policy output is opinionated, we can end up in the same sub-branches. see: our promotion matches containing duplicate games
- INVESTIGATION: check this position, where 1M_v009 at 20K playouts thinks White's best move is to move the left pusher to the edge on the bottom left:
      . . 
    . . .
    . w w .
    . b W .
    W B W B!
    . . B .
      . b .
      . .
    White to move | 1 moves left
        - of course, black then fails to see the win-in-two
			  . .
			. . .
			B w w .
			. b . .
			. B W!B
			. . W .
			  . . .
			  W b
        - The bad move is in third place with low policy prior and 1/4 the visits of the top, reasonable move
        - Black is also failing to see a win-in-1 at 20K playouts that it spots at 1K because bad moves are catching up with more playouts
            - It might just be that it's also a guaranteed win, and the slower move has a slightly higher policy prior
            - With 0 moves it only has two push options, and value is set correctly. Win-in-1 had a 40/60 incorrect policy.
            - Seeing two problems even at 20K playouts
                - Extremely high values getting overpowered by high policy priors
                    - With ~50 root children, about half are only getting a single visit. We want a higher exploration factor, but then the policy is too extreme
                    - The policy of 1M_v9 is just too extreme
                        - [(Move@5>21, 9639, '0.97', '0.13%'), (Move@25>13, 5747, '0.04', '61.19%'), ...
                        - The top move is a guaranteed win and the second is essentially a draw, but because it has 470x the policy prior, it is still eating half the visits
                        - Logits are 5.22 and -0.89, respectively. e^x ain't gonna fix that. How did this logit get so high?
                        - There's even a 7.66 logit, but it seems to be for an illegal move.
                        - Of course, if the high logit were for a good move, I'd be thrilled to see it.
                        - If we're fairly confident about the value, we should stop caring about the policy prior.
                        - Part of this is the network's fault, but part is the bound calculation
                - MCTS not finding a high expected value for a win-in-2 (three pushers ready to move into winning position)
                    - it's seeing it if it's literally the only move
                    - looks like this is just caused by the first problem: a non-winning move has a huge policy prior
                    - somehow, lowering exploration is making it worse
                    - okay, we were flipping value estimates from the network incorrectly
                    - win-in-2s aren't able to overcome low policy priors unless there are very few children
                    - tested with NullNet, which had all 3 winning moves top at 20K playouts, had to increase exploration to ~50 to make it "vanilla" MCTS
- 1M2 (same topology)
    - Hoping that the too-wide range of priors was the result of low-playout games which were dominated by policy priors, leading to a self-enforcing cycle.
    - Restarting with 1024 playout training, also fixed the incorrect value prior bug
        - Generated 6412 training examples in 2790s.
    - Re: policy logits: in training examples, pushes will always have high logits since they're sometimes the only legal moves. How do we prevent them from beating moves?
    - Terrible first run: 53%, 30%, 30%, 41%, 34%, 36%. All unique games.
- 320K
    - Last run was so disastrous, need to be able to iterate faster
    - Trying to switch to cross entropy or K1
    - First eval 46%, policy loss is on the rise. Could still be correct: better value estimates mean better policy to compare against.
    - Then down to 38%: loss is actually looking better than the start now, but it's seemingly not translating to good play.
    - Ran on one batch of training data, value and policy loss were indeed both going down. Might be an MCTS bug.
    - Removed softmax on policy logits in MCTS, using e^x instead, set exploration to the more standard sqrt(2).
    - Still consistently getting sub 40% with no wins. At least it's reproducible!
    - Overnight checkpoint performing about the same as 320v1 at the triple black pusher test case
        - But the slightly different case .....Bww......BWB..W.b..Wb has much worse policy priors. The white version doesn't have this problem.
    - Found a bug in PFState.to_tensor
    - Now reverting to old network on promotion match loss
    - Policy loss is too low
    - 320K_bad is placing its pieces on the edge
    - It also has hefty priors for insta-losing moves
    - Wait... does flipping the policy vector for push moves actually work?
- TODO
    - don't flip the board, just the bit vector representations
    - more profiling
    - parallelize promotion games, include game length cap
    - seeing anecdotal evidence that the training values should incorporate the MCTS result, not just the game result
    - try debugging and tuning hyperparameters on an even smaller network and game board
    - some kind of "curriculum": start later into piece placement?