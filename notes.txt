- CPU/GPU issues
    - Running the network just doesn't take that long, so using the GPU is currently a waste.
        - That means parallelizing MCTS runs to batch forward passes is also not helpful.
    - The CPU is the bottleneck
    - Surprisingly, movegen isn't that bad. Right now the killer is converting states to tensors
        - Maybe I shouldn't be surprised: replacing random playouts with value predictions hugely reduces the load on movegen
        - Each modification to the tensor, or concatenating the whole thing, is taking way longer than expected
        - Improved this by using numpy arrays as an intermediate
        - Still taking longer than you'd think! Needs further optimization
        - Should probably use np all the way through PFState etc
        - In general, we'll want to store game states in a way that's easy for both movegen and conversion to network input
    - I'm realizing that you either have to train on CPU, or do a lot of shuffling of tensors around: gotta make parallel worth it somehow
- 750K (160, 512, 512, 512, 807)
    - It definitely learned to place pieces on the line, and usually to move them away from the edge
    - Most of the bugs seem to be fixed, but there's still the occasional do-nothing game where the agents somehow can't see the win-in-2.
    - These games seem like they take longer than usual to reach 200 history.
    - 750K_v006 policy priors are in the approximate range [-3, 3]: since these are multipliers, they should really be non-negative.
        - This is resulting in a very narrow MCTS.
    - Now using most robust child (best move only) in test matches instead of weighted by visits.
        - Oop, wait: we can't actually do this, since MCTS and the networks are both deterministic. We could try lowering the temperature...?
    - The network is probably just not deep enough. Let's do a run with a bigger net...
    - I'd forgotten to apply a softmax to the policy logits when instantiating MCTS: oops!
        - Also forgot to flip everything if the root state is black to play
- 1700K (160, 1024, 512, 512, 256, 256, 512, 807)
    - Upped exploration (aka c_PUCT in the AlphaZero paper) to 10, but it still seems like it's too low (now to 50)
    - Modifications made to network output before using them in MCTS need to be parallelizable.
        - Example: rotating the board for black is cute, but it means that half of the policy outputs need to be rearranged before storing in MCTS tree.
            - We could maybe parallelize this by sorting network inputs by state.white_to_play, so at least the policies that need rearranging are all together
            - But then it becomes annoying to get them back to the MCTS instance they belong to.
    - another round of profiling
        - CPU parallelism=1: generated 200 examples in 23.495507 seconds. (.117s per)
        - CPU parallelism=20: generated 4000 examples in 228.565861 seconds. (.057s per)
        - GPU parallelism=1: generated 200 examples in 39.540972 seconds. (.197s per)
        - GPU parallelism=20: generated 4000 examples in 343.205210 seconds. (.086s per)
- 1M (160, 512, 256, 256, 256, 256, 512, 807)
    - Lowered exploration back down to 5: if MCTS is largely driven by policy, then the result after running will be too similar and we have no delta to train on
    - Added momentum to SGD, lowered learning rate
    - check game states that are close to a win: make sure even a crappy network can find the win with enough playouts
        - white to play, has to move a round piece out of the way so a pusher can come in and win
            - 1M_v006 takes 1-2K playouts to see the line, fighting against policy: the move it lands on has 1%, and a non-winning move has 51%.
            - Uh oh: 1M_v009 needs 2-4K to see the line. At least policy is less confident about the wrong move at 49%...?
    - random thought: seems a bit of a waste to have training games and eval games. why not have all games be both?
        - round robin, train with jitter(?) on all games to create the next generation
    - 1Mv8 is about even with 1700Kv3, 65% against 750Kv6
    - Switching from 128 -> 1024 playout training games: ~12800 training examples in ~1000s. (NOTE: looks like I accidentally failed to change this.)
    - Suddenly, white is winning almost all promotion games. Are we overconcentrating our visits, causing duplicate games?
        - Promotion games are still only 128 playouts, which can be policy dominated.
        - Increasing playouts should also result in more unique games if only because it should result in longer games.
        - 1Mv9 vs 1700Kv3
            - @ 128 playouts: New network won 68 of 100 games (68.00%). 57 unique games. Average length 22.61. White won 80/100.
            - @ 256 playouts: New network won 70 of 100 games (70.00%). 63 unique games. Average length 26.93. White won 78/100.
            - @ 1024 playouts: New network won 80 of 100 games (80.00%). 67 unique games. White won 70/100.
        - It might not actually be that bad to have duplicate games, unless the reason is that the networks are stupidly losing immediately.
            - It's not great: even if we're seeing 60-70/100 unique games, they likely only differ at the very end
        - Switching to 256-visit promotion games: more playouts seems to give better signal
            - That's if you go by my biased definition of "better signal": the newer network winning more
    - 1M_v009 has learned that pushing is good, and isn't really moving much...
    - After v9, every new network is terrible, getting as low as 28% winrate.
    - A "network" that outputs all zeros is going to be better than early networks since it's vanilla MCTS, versus MCTS with random errors
        - We should use this to generate training data until a network can beat it
        - But is that true? Apparently not.
            - @ 256 playouts:
                - vs new network 1Mv1: New network won 55 of 100 (55.00%). 100 unique games. Average length 13.71. White won 69/100.
                - vs new network 1Mv9: New network won 81 of 100 (81.00%). 100 unique games. Average length 20.46. White won 51/100.
            - @ 512 playouts:
                - vs new network 1Mv1: New network won 58 of 100 (58.00%). 100 unique games. Average length 14.31. White won 68/100.
                - vs new network 1Mv9: New network won 83 of 100 (83.00%). 100 unique games. Average length 17.92. White won 49/100.
        - It's not really vanilla MCTS, since there are no actual rollouts, just zeros. Unlikely to reach winning states at much depth.
    - self-play is redoing a ton of work. can't we just build a single giant MCTS tree and any node with enough visits can serve as a training example?
        - would want to explore wider than a typical MCTS search since we're not just looking for winners, but really just any example
        - therefore we'd want to differentiate between visits made on UCT grounds vs. those just for funsies
        - we _could_ try to cut out some of the exploitation that hits leaf nodes over and over, but could also just optimize tree traversal
        - this gets us more policy examples, but what about value?
            - once we have enough policy examples, we can play those games out normally
        - thinking about it more: "a ton of work" is not accurate. the high branching factor of the game begs to differ.
            - but then again, if the policy output is opinionated, we can end up in the same sub-branches. see: our promotion matches containing duplicate games
- INVESTIGATION: check this position, where 1M_v009 at 20K playouts thinks White's best move is to move the left pusher to the edge on the bottom left:
      . . 
    . . .
    . w w .
    . b W .
    W B W B!
    . . B .
      . b .
      . .
    White to move | 1 moves left
        - of course, black then fails to see the win-in-two
			  . .
			. . .
			B w w .
			. b . .
			. B W!B
			. . W .
			  . . .
			  W b
        - The bad move is in third place with low policy prior and 1/4 the visits of the top, reasonable move
        - Black is also failing to see a win-in-1 at 20K playouts that it spots at 1K because bad moves are catching up with more playouts
            - It might just be that it's also a guaranteed win, and the slower move has a slightly higher policy prior
            - With 0 moves it only has two push options, and value is set correctly. Win-in-1 had a 40/60 incorrect policy.
            - Seeing two problems even at 20K playouts
                - Extremely high values getting overpowered by high policy priors
                    - With ~50 root children, about half are only getting a single visit. We want a higher exploration factor, but then the policy is too extreme
                    - The policy of 1M_v9 is just too extreme
                        - [(Move@5>21, 9639, '0.97', '0.13%'), (Move@25>13, 5747, '0.04', '61.19%'), ...
                        - The top move is a guaranteed win and the second is essentially a draw, but because it has 470x the policy prior, it is still eating half the visits
                        - Logits are 5.22 and -0.89, respectively. e^x ain't gonna fix that. How did this logit get so high?
                        - There's even a 7.66 logit, but it seems to be for an illegal move.
                        - Of course, if the high logit were for a good move, I'd be thrilled to see it.
                        - If we're fairly confident about the value, we should stop caring about the policy prior.
                        - Part of this is the network's fault, but part is the bound calculation
                - MCTS not finding a high expected value for a win-in-2 (three pushers ready to move into winning position)
                    - it's seeing it if it's literally the only move
                    - looks like this is just caused by the first problem: a non-winning move has a huge policy prior
                    - somehow, lowering exploration is making it worse
                    - okay, we were flipping value estimates from the network incorrectly
                    - win-in-2s aren't able to overcome low policy priors unless there are very few children
                    - tested with NullNet, which had all 3 winning moves top at 20K playouts, had to increase exploration to ~50 to make it "vanilla" MCTS
- 1M2 (same topology)
    - Hoping that the too-wide range of priors was the result of low-playout games which were dominated by policy priors, leading to a self-enforcing cycle.
    - Restarting with 1024 playout training, also fixed the incorrect value prior bug
        - Generated 6412 training examples in 2790s.
    - Re: policy logits: in training examples, pushes will always have high logits since they're sometimes the only legal moves. How do we prevent them from beating moves?
    - Terrible first run: 53%, 30%, 30%, 41%, 34%, 36%. All unique games.
- 320K
    - Last run was so disastrous, need to be able to iterate faster
    - Trying to switch to cross entropy or K1
    - First eval 46%, policy loss is on the rise. Could still be correct: better value estimates mean better policy to compare against.
    - Then down to 38%: loss is actually looking better than the start now, but it's seemingly not translating to good play.
    - Ran on one batch of training data, value and policy loss were indeed both going down. Might be an MCTS bug.
    - Removed softmax on policy logits in MCTS, using e^x instead, set exploration to the more standard sqrt(2).
    - Still consistently getting sub 40% with no wins. At least it's reproducible!
    - Overnight checkpoint performing about the same as 320v1 at the triple black pusher test case
        - But the slightly different case .....Bww......BWB..W.b..Wb has much worse policy priors. The white version doesn't have this problem.
    - Found a bug in PFState.to_tensor
    - Now reverting to old network on promotion match loss
    - Policy loss is too low
    - 320K_bad is placing its pieces on the edge
    - It also has hefty priors for insta-losing moves
        - These priors persist even after flipping colors, suggesting that flipping isn't to blame. Testing anyway.
    - Wait... does flipping the policy vector for push moves actually work? It should, given the (up, left, right, down) order.
    - Got rid of board flipping: no reason for the network to learn that your own pieces are usually at the top.
        - Maybe actively worse: flipping removes many symmetries, which AZ adds intentionally
    - Maybe failing to find a v2 network is expected behavior: a close-to-zero network is unbiased no-rollout MCTS, lets win values control the entire search
    - We should generate training data from NullNet until the network is sufficiently improved to make the slowdown worth it
    - Profiled NullNet generation: large percentage of time spent just in PFMove.place() calling cls(...)?
        - Current: Generated 6422 training examples in 416s.
        - Trying to use __slots__, slots=True, NamedTuple, etc was all slower somehow
    - Reading the AlphaZero paper:
        - their self-play games are from the latest network
        - training was learning rate 0.2->0.02->0.002->0.0002, 700K batches of 4096 examples @800 playouts
        - self-play was done on TPUs
        - root nodes added Dirichlet noise to policy priors
    - Even after 6 low-percent "wins" against previous networks, 320Kv7 is only 50% against NullNet
    - 320Kv7 @4k playouts as Black after a good placement from White wants to place its pieces at 25, 24, and 23, with negative priors, but high expected value
    - @512 playouts, training games are just very poor quality from NullNet, and in general. we need ~2k before we see even a glimmer of intelligence.
        - temp = 1 for training games is just wrong: we should pick the highest visit count much more often
        - lesson: actually inspect training data
        - Updated to 2048 playouts (still using NullNet to train): Generated 6409 training examples in 1626s.
    - Reading the KataGo paper:
        - They might just be using policy priors instead of e^x? And they have exploration at 1.1.
        - Also Dirichlet noise
        - Started with a smaller network, retrained each larger network on the same data until it caught up in loss
        - SGD with 0.9 momentum, batch size 256
        - "Playout cap randomization": Instead of always running a full MCTS in self-play, 75% of the time they use 1/5 the playouts, but don't use those outcomes as training data.
            - These fast moves disable noise and minimize exploration
        - "Forced Playouts and Policy Target Pruning": after N playouts in self-play, continue running until all root children have a minimum visit count
            - Then, modify the policy target to diminish moves that are still bad after this forced expansion
        - Their network has another policy head for expected response moves from the opponent, which is only used in self play.
    - Value loss is still basically not moving even after hundreds of batches. There are enough obvious cases that the net should be able to learn: something bad is happening.
        - Okay, here's a problem: when generating examples, I'm treating the value output as "did the current player win (1), or lose (-1)?"
        - But in MCTS, it's "did Black win (-1), or White win (1)?"
        - Obviously we need to treat these consistently, and besides, the network doesn't even know what "White" and "Black" are.
- Testing mate-in-one-detection net on 1M examples (about 1 in 7 are mates in one)
    - SGD is still faster than Adam
    - Starting to seem like it's pretty inefficient running backprop on each training example just once
        - KataGo also keeps a "window" of examples from prior nets around.
            - The farther into training, the larger the window can be since even older models + playouts are stronger than the current net.
            - Alpha_Go_ Zero used a simpler window of just the most recent 500K games.
    - 117K batch1024 lr0.2: 0.0004 after ~40 epochs
        - lr decay .99 ** epoch: .00033 after 60 epochs
        X batch128: .00026 after 50 epochs, but twice as slow as batch1024. reverting...
        X I was missing a ReLU after the first layer (also in the "production" net) but adding it isn't having the effect I'd have imagined
        - batchnorm (up to 119K params): .00026 after 25 epochs, .0002 after 40, .00018 after 60
            - batchnorm theoretically goes before nonlinearity, but it's working better for me after, which some have observed: yet another hyperparameter
        X 3 more middle layers (up to 152K params): worse than the previous run
        X removing the last layer (91K params): quick to .0025 after 20 epochs, then starts flailing
        - shrink and restructure (64K params): .00023 at 40 epochs, sticks
        X try a huge run (811K params, batch128): overfit...? about the same as previous bests
        X training at half precision brought loss up by about 25%, not too bad, but no more performant
- Testing separate network trained on NullNet dataset: 267K params
    - Generating ~1K examples / 80sec
    - 20K examples can get test loss from 1.63 to 1.51
    - batchnorm is doing something screwy, making loss explode to infinity. Could just be that the dataset is too small...?
    - 100K examples, loss down to 1.36, but it still rebounds and starts increasing to 1.7+ after ~15 epochs: probably overfitting
        - Train and test loss descent together, then start to diverge as train loss plummets to 1.10
        - batchnorm seems to make it even better at overfitting, train loss 0.82 and falling while test loss > 1.6. At least test policy loss is decreasing! (2.8 -> 2.7)

- TODO
    - parallelize promotion games
    - seeing anecdotal evidence that the training values should incorporate the MCTS result, not just the game result
    - "curriculum" based on high-loss game states